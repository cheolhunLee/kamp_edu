{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4512524f",
   "metadata": {},
   "source": [
    "# PEFT: LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a33f8c9",
   "metadata": {},
   "source": [
    "* LoRA 기반 PEFT를 학습하기 위한 예제 데이터셋으로 한국어 데이터셋인 koAlpaca의 100행만 추출하여 사용함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb647a26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "instruction",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "output",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "f88a19e2-a18f-46ea-a8ee-5dd389f7c8c1",
       "rows": [
        [
         "0",
         "건강을 유지하기 위한 세 가지 팁을 알려주세요.",
         "세 가지 팁은 아침식사를 꼭 챙기며, 충분한 수면을 취하고, 적극적으로 운동을 하는 것입니다."
        ],
        [
         "1",
         "세 가지 기본 색은 무엇인가요?",
         "기본 색은 빨강, 파랑, 노랑입니다."
        ],
        [
         "2",
         "원자의 구조를 설명하세요.",
         "원자는 양성자, 중성자, 전자로 구성되어 있으며, 양성자와 중성자는 원자핵 안에 있고 전자는 주변에 있습니다."
        ],
        [
         "3",
         "대기 오염을 어떻게 줄일 수 있나요?",
         "대기 오염을 줄이기 위해 자동차 대신 대중교통을 이용하고, 미세먼지 배출량을 줄이기 위해 차량 정기 검사를 받아야 합니다. 또한 에너지 절약을 위해 전기를 아끼고, 나무를 심어 대기 오염을 줄일 수 있습니다."
        ],
        [
         "4",
         "어려운 결정을 내려야 했던 때를 설명하세요.",
         "제가 어려운 결정을 내려야 했던 때는 대학원 졸업 후 직장을 찾아야 했던 때입니다. 다양한 선택지가 있었지만 나의 관심사와 비전에 가장 부합하는 경력을 뽑는데 어려움이 있었습니다. 이 때 여러 사람들의 조언과 현재 상황에 대한 깊은 고민 끝에 적절한 선택을 내리게 되었습니다."
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>건강을 유지하기 위한 세 가지 팁을 알려주세요.</td>\n",
       "      <td>세 가지 팁은 아침식사를 꼭 챙기며, 충분한 수면을 취하고, 적극적으로 운동을 하는...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>세 가지 기본 색은 무엇인가요?</td>\n",
       "      <td>기본 색은 빨강, 파랑, 노랑입니다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>원자의 구조를 설명하세요.</td>\n",
       "      <td>원자는 양성자, 중성자, 전자로 구성되어 있으며, 양성자와 중성자는 원자핵 안에 있...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>대기 오염을 어떻게 줄일 수 있나요?</td>\n",
       "      <td>대기 오염을 줄이기 위해 자동차 대신 대중교통을 이용하고, 미세먼지 배출량을 줄이기...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>어려운 결정을 내려야 했던 때를 설명하세요.</td>\n",
       "      <td>제가 어려운 결정을 내려야 했던 때는 대학원 졸업 후 직장을 찾아야 했던 때입니다....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  instruction  \\\n",
       "0  건강을 유지하기 위한 세 가지 팁을 알려주세요.   \n",
       "1           세 가지 기본 색은 무엇인가요?   \n",
       "2              원자의 구조를 설명하세요.   \n",
       "3        대기 오염을 어떻게 줄일 수 있나요?   \n",
       "4    어려운 결정을 내려야 했던 때를 설명하세요.   \n",
       "\n",
       "                                              output  \n",
       "0  세 가지 팁은 아침식사를 꼭 챙기며, 충분한 수면을 취하고, 적극적으로 운동을 하는...  \n",
       "1                               기본 색은 빨강, 파랑, 노랑입니다.  \n",
       "2  원자는 양성자, 중성자, 전자로 구성되어 있으며, 양성자와 중성자는 원자핵 안에 있...  \n",
       "3  대기 오염을 줄이기 위해 자동차 대신 대중교통을 이용하고, 미세먼지 배출량을 줄이기...  \n",
       "4  제가 어려운 결정을 내려야 했던 때는 대학원 졸업 후 직장을 찾아야 했던 때입니다....  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_path = './datasets/alpaca_data.json'\n",
    "df = pd.read_json(data_path)\n",
    "df = df[['instruction', 'output']]\n",
    "df = df.iloc[:100]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a878a4fa",
   "metadata": {},
   "source": [
    "* Pretrained LLM 들은 명시적인 Prompt 구조에서 입력을 해석하고 응답을 생성하도록 학습되어 있음.\n",
    "* 때문에, 본격적인 모델 학습에 앞서 Llama3.1에서 지원하는 Instruct 구조에 맞게 데이터 형식을 수정해야 모델이 적절하게 문맥을 이해하여 안정적인 학습이 가능함.\n",
    "* 각 데이터는 ‘user(사용자)’와 ‘assistant’(모델 응답)의 대화 형식으로 구성되어짐."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "292d8659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82e7a0616cae43d4b3871fffe493eaa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def format_example(row):\n",
    "    return {\n",
    "        'text': f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    " \n",
    "        You are a helpful assistant<|eot_id|>\\n<|start_header_id|>user<|end_header_id|>\n",
    " \n",
    "        {row['instruction']}<|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>\n",
    " \n",
    "        {row['output']}<|eot_id|>\"\"\"\n",
    "    }\n",
    " \n",
    "# 판다스 데이터프레임을 데이터셋으로 변환하고, 포맷팅 함수 적용\n",
    "dataset = Dataset.from_pandas(df)\n",
    "dataset = dataset.map(format_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed121d8",
   "metadata": {},
   "source": [
    "* 사전학습된 모델을 불러오기 위해 Hugging Face에서 제공하는 AutoModelForCausalLM, AutoTokenizer를 import \n",
    "* AutoModelForCausalLM: 인과 언어 모델(주로 텍스트 생성에 쓰임)을 로드\n",
    "* AutoTokenizer: 해당 모델에 맞는 토크나이저를 자동으로 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c99b8ae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6e3b47dd7b747cc95dd0589977b6d56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek-R1-Distill-Llama-8B 모델이 성공적으로 로드되었습니다.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# DeepSeek-R1-Distill-Llama-8B 모델 로드\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, torch_dtype=\"auto\", device_map=\"auto\")\n",
    "\n",
    "print(\"DeepSeek-R1-Distill-Llama-8B 모델이 성공적으로 로드되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ae37aa",
   "metadata": {},
   "source": [
    "* 받은 모델을 간단히 테스트함. 질문을 tokenizer로 토큰으로 변환한 후 모델에 입력하고, 출력을 다시 tokenizer로 디코딩하여 모델 응답을 얻음\n",
    "* 한국어 질문에 대해서 중국어로 답변을 내놓으므로 정확한 답이 될수 없음. 이문제를 LoRA로 해결해봄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1de0fcf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      " \n",
      "You are a helpful assistant<|eot_id|>\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      " \n",
      "건강을 유지하기 위한 세 가지 팁을 알려주세요.<|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "嗯，用户让我给出三个保持健康的建议。好的，首先得考虑用户的需求，可能他们想知道如何更好地维护自己的身体和心理健康。保持健康很重要，所以我要提供实用的建议。\n",
      "\n",
      "首先，饮食方面肯定是关键。建议用户多吃蔬菜和水果，保持均衡的营养。这能帮助他们获得必要的维生素和矿物质，增强免疫力。然后，运动也很重要，建议每周至少做几次有氧运动，比如跑步或游泳，这样有助于保持心肺功能健康。还有，强调定期体检，早期发现问题，预防疾病，这也是很重要的。\n",
      "\n",
      "另外，心理健康也不能忽视。建议用户学会放松，比如冥想或深呼吸，这样能缓解压力。保持良好的人际关系也很重要，帮助他们建立一个支持性的社交网络。最后，充足\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    " \n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=256)\n",
    " \n",
    "def generate_and_stop(prompt):\n",
    "    result = pipe(f\"{prompt}\")[0]['generated_text']\n",
    "    return result\n",
    " \n",
    "prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    " \n",
    "You are a helpful assistant<|eot_id|>\\n<|start_header_id|>user<|end_header_id|>\n",
    " \n",
    "건강을 유지하기 위한 세 가지 팁을 알려주세요.<|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    " \n",
    "print(generate_and_stop(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ed9333",
   "metadata": {},
   "source": [
    "* PEFT의 파라미터를 설정 \n",
    "* LoraConfig()를 통해서 LoRA 모델의 특정 파라미터만 조정하여 미세 조정(PEFT)을 수행\n",
    "* lora_dropout은 과적합을 방지하기 위해 레이어의 가중치를 드롭아웃할 확률\n",
    "* prepare_model_for_kbit_training을 통해 메모리를 절약하면서도 성능을 유지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b7a86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "peft_params = LoraConfig(\n",
    "    lora_alpha=16, #scaling factor\n",
    "    lora_dropout=0.05, #과적합 방지\n",
    "    r=16, #low-rank 차원 수\n",
    "    bias=\"none\", #bias 항에 대해서는 학습을 하지 않음\n",
    "    task_type=\"CAUSAL_LM\", #Causal Language Model\n",
    "    target_modules=['k_proj', 'q_proj', 'v_proj', 'o_proj'] #LoRA/DoRA가 적용될 대상 레이어\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model, 8) #model을 8 bit 양자화 상태에서 학습 가능하도록 준비\n",
    "model = get_peft_model(model, peft_params) #8bit model에 대해 PEFT 버전 모델로 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d20f2b",
   "metadata": {},
   "source": [
    "* 모델 학습 관련 파라미터들을 설정하기 위해 transformers 라이브러리의 TrainingArguments 클래스를 import\n",
    "* 이 클래스는 Trainer 또는 SFTTrainer (Supervised Fine-Tuning Trainer) 와 같은 학습 \u000b클래스에 전달되어 학습 전반을 제어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b6b8c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments \n",
    "\n",
    "training_params = TrainingArguments(\n",
    "    output_dir=\"./results\", #학습된 모델 체크포인트, config, tokenizer 등을 저장할 디렉토리 경로\n",
    "    logging_dir=\"./logs\", #학습 과정의 손실, 정확도, 학습률 등의 로그를 저장할 디렉토리 경로\n",
    "    logging_steps=10, #몇 스텝마다 로그를 저장할지 설정\n",
    "    save_steps=1000, #몇 스텝마다 모델을 저장할지 설정\n",
    "    num_train_epochs=50, #전체 학습 데이터셋을 몇 번 반복(epoch)할지를 지정                   \n",
    "    per_device_train_batch_size=1, #GPU(혹은 CPU)가 한 번에 처리할 샘플의 수       \n",
    "    gradient_accumulation_steps=8, #8번의 스탭마다 1번 파라미터를 업데이트\n",
    "    learning_rate=2e-4, #학습률이 0.0002              \n",
    "    fp16=True, #학습 시 일부 연산을 16 bit로 수행하여 메모리 절약 (원래는 32 bit)          \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62c9e47",
   "metadata": {},
   "source": [
    "* SFTTrainer를 사용해 학습 설정\n",
    "    * train_dataset: 보통 JSON 형태로 prompt + response 를 합쳐 text로 구성\n",
    "    * peft_config: LoRA 적용 시 사용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c23d200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beb978d4a3cd4231a961dc0b08f14552",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting train dataset to ChatML:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3da999287d04fae9bcdf05aaeb61ef9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e7c83d3263643dbba5837ca749dd2db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e542c97bd5c8425c9e016f8faa06517a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model, #학습시킬 모델\n",
    "    train_dataset=dataset, #학습에 사용할 데이터셋\n",
    "    args=training_params, #학습 설정 객체(TrainingArguments)\n",
    "    peft_config=peft_params, #PEFT 관련 설정 객체(LoraConfig)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b89819",
   "metadata": {},
   "source": [
    "* 학습 파라미터인 num_train_epochs는 50인데, 배치 사이즈(한 스텝에 \u000b처리하는 샘플 수)에 따라 600 Step으로 쪼개어짐 \n",
    "* 50 Epoch (한 데이터를 50번 반복해서 학습) 한 결과 Loss 값이 4에서 \u000b(초기 10 Step)에서 0.31(최종 600 Step)까지 감소함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4c91763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='600' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [600/600 7:13:50, Epoch 46/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.043500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.344800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.924500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.761600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.420000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.346000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.083400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.077600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.873400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.791100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.699100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.643200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.566500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.501200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.475900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.438200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.423200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.400500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.400400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.386800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.370300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.364900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.350700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.372600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.334700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.342000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.333200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.340100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.330700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.346000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.338400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.306800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.342700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.332300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.322000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.322500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.325300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.327100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.332000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.323400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.318700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.331800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.325100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.324100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.304700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.317000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.335300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.312600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.311500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.333900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.305900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.320200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.311700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.317500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.329000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.312300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.324700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.322700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.315600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=600, training_loss=0.5788791978359222, metrics={'train_runtime': 26139.4439, 'train_samples_per_second': 0.191, 'train_steps_per_second': 0.023, 'total_flos': 1.792310927806464e+16, 'train_loss': 0.5788791978359222})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bafac3bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./Deepseek-R1-Distill-Llama-8b_LoRA_alpace_50\\\\tokenizer_config.json',\n",
       " './Deepseek-R1-Distill-Llama-8b_LoRA_alpace_50\\\\special_tokens_map.json',\n",
       " './Deepseek-R1-Distill-Llama-8b_LoRA_alpace_50\\\\tokenizer.json')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6. 모델 저장\n",
    "model.save_pretrained(\"./Deepseek-R1-Distill-Llama-8b_LoRA_alpace_50\")\n",
    "tokenizer.save_pretrained(\"./Deepseek-R1-Distill-Llama-8b_LoRA_alpace_50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3d23d7",
   "metadata": {},
   "source": [
    "* 미세조정된 모델은 같은 한국어 질문에 대해서 중국어가 아닌 한국어로 답변했고, 그 결과가 학습 데이터와 유사하게 나타남\n",
    "* 하지만, 문장 속의 표현이 어색하고 어순이 잘못되어 있음 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e428810e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DeepseekV3ForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'Llama4ForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'Phi4MultimodalForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      " \n",
      "You are a helpful assistant<|eot_id|>\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      " \n",
      "건강을 유지하기 위한 세 가지 팁을 알려주세요.<|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "세 가지 팁 중 하나는 식습관을 지키는 건강을 유지하기 위한 중요한 요소입니다. 두 번째 팁은 규칙적으로 수면을 유지하며 세 번째 팁은 적극적으로 운동을 하는 것입니다.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    " \n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=256)\n",
    " \n",
    "def generate_and_stop(prompt):\n",
    "    result = pipe(f\"{prompt}\")[0]['generated_text']\n",
    "    return result\n",
    " \n",
    "prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    " \n",
    "You are a helpful assistant<|eot_id|>\\n<|start_header_id|>user<|end_header_id|>\n",
    " \n",
    "건강을 유지하기 위한 세 가지 팁을 알려주세요.<|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    " \n",
    "print(generate_and_stop(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1801b49d",
   "metadata": {},
   "source": [
    "# PEFT: DoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380fa985",
   "metadata": {},
   "source": [
    "* 기존 LoRA의 학습 코드에서  LoraConfig() 함수 속에 use_dora 매개변수만 추가하여 use_dora=True 꼴로 작성해주면, LoRA 대신 DoRA를 사용할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b166197a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "peft_params = LoraConfig(\n",
    "    use_dora=True, #LoRA가 아닌 DoRA를 사용함.\n",
    "    lora_alpha=16, #scaling factor\n",
    "    lora_dropout=0.05, #과적합 방지\n",
    "    r=16, #low-rank 차원 수\n",
    "    bias=\"none\", #bias 항에 대해서는 학습을 하지 않음\n",
    "    task_type=\"CAUSAL_LM\", #Causal Language Model\n",
    "    target_modules=['k_proj', 'q_proj', 'v_proj', 'o_proj'] #LoRA/DoRA가 적용될 대상 레이어\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model, 8) #model을 8 bit 양자화 상태에서 학습 가능하도록 준비\n",
    "model = get_peft_model(model, peft_params) #8bit model에 대해 PEFT 버전 모델로 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8ce7de",
   "metadata": {},
   "source": [
    "* LoRA의 학습 결과와 비교했을 때, LoRA로 미세조정한 LLM의 응답은 어색한 문장 구조를 나타내고 있지만, DoRA로 미세조정한 LLM의 응답은 학습 데이터와 거의 일치하는 출력 결과를 나타냄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4d120d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a74cb242223c4e858f48e2f1cf5c878d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "model_path = \"Deepseek-R1-Distill-Llama-8b_LoRA_alpace_50\"\n",
    "\n",
    "peft_config = PeftConfig.from_pretrained(model_path)\n",
    "\n",
    "# 2. Base 모델 로드 (LoRA를 적용할 원본 모델)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(peft_config.base_model_name_or_path)\n",
    "\n",
    "# 3. LoRA 가중치를 base model에 적용\n",
    "model = PeftModel.from_pretrained(base_model, model_path)\n",
    "\n",
    "# 4. Tokenizer 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "640e2c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DeepseekV3ForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'Llama4ForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'Phi4MultimodalForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      " \n",
      "You are a helpful assistant<|eot_id|>\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      " \n",
      "건강을 유지하기 위한 세 가지 팁을 알려주세요.<|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      " 세 가지 팁 중 하나는 세면을 시간적으로 관리하는 방법이입니다. 두 번째 팁은 꾸준한 수면이 건 건강에 큰 영향을 미치도록 있습니다. 세 번째 팁은 적절한 식습관을 유지하는 방법이입니다.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    " \n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=256)\n",
    " \n",
    "def generate_and_stop(prompt):\n",
    "    result = pipe(f\"{prompt}\")[0]['generated_text']\n",
    "    return result\n",
    " \n",
    "prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    " \n",
    "You are a helpful assistant<|eot_id|>\\n<|start_header_id|>user<|end_header_id|>\n",
    " \n",
    "건강을 유지하기 위한 세 가지 팁을 알려주세요.<|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    " \n",
    "print(generate_and_stop(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "719a7186",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DeepseekV3ForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'Llama4ForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'Phi4MultimodalForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      " \n",
      "You are a helpful assistant<|eot_id|>\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      " \n",
      "대기 오염을 어떻게 줄일 수 있나요?<|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      " 줄이는 대로 도rive하고, 대기 오염 방지를 위해 바다에 폐기물을 던지하지 않고 처리하고, 전기 발전소를 미세먼지로 효율이 고쳐져는 등 대기 오염을 줄이기 위해 고쳐져 있습니다.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    " \n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=256)\n",
    " \n",
    "def generate_and_stop(prompt):\n",
    "    result = pipe(f\"{prompt}\")[0]['generated_text']\n",
    "    return result\n",
    " \n",
    "prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    " \n",
    "You are a helpful assistant<|eot_id|>\\n<|start_header_id|>user<|end_header_id|>\n",
    " \n",
    "대기 오염을 어떻게 줄일 수 있나요?<|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    " \n",
    "print(generate_and_stop(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f7b242f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9738d70867e43cdac17882d184588b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "model_path = \"Deepseek-R1-Distill-Llama-8b_DoRA_alpaca_50\"\n",
    "\n",
    "peft_config = PeftConfig.from_pretrained(model_path)\n",
    "\n",
    "# 2. Base 모델 로드 (LoRA를 적용할 원본 모델)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(peft_config.base_model_name_or_path)\n",
    "\n",
    "# 3. LoRA 가중치를 base model에 적용\n",
    "model = PeftModel.from_pretrained(base_model, model_path)\n",
    "\n",
    "# 4. Tokenizer 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "181c496d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DeepseekV3ForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'Llama4ForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'Phi4MultimodalForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      " \n",
      "You are a helpful assistant<|eot_id|>\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      " \n",
      "건강을 유지하기 위한 세 가지 팁을 알려주세요.<|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "    세 가지 팁은 아침식사를 꼭 챙기며, 충분한 수면을 취하고, 적극적으로 운동을 하는 것입니다.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    " \n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=256)\n",
    " \n",
    "def generate_and_stop(prompt):\n",
    "    result = pipe(f\"{prompt}\")[0]['generated_text']\n",
    "    return result\n",
    " \n",
    "prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    " \n",
    "You are a helpful assistant<|eot_id|>\\n<|start_header_id|>user<|end_header_id|>\n",
    " \n",
    "건강을 유지하기 위한 세 가지 팁을 알려주세요.<|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    " \n",
    "print(generate_and_stop(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9324b75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DeepseekV3ForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'Llama4ForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'Phi4MultimodalForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      " \n",
      "You are a helpful assistant<|eot_id|>\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      " \n",
      "대기 오염을 어떻게 줄일 수 있나요?<|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "assistant\n",
      " \n",
      "대기 오염을 줄이기 위해 자동차 대신 대중교통을 이용하고, 미세먼지 배출량을 줄이기 위해 차량 정기 검사를 받아야 합니다. 또한 에너지 절약을 위해 전기를 아끼고, 나무를 심어 대기 오염을 줄일 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    " \n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=256)\n",
    " \n",
    "def generate_and_stop(prompt):\n",
    "    result = pipe(f\"{prompt}\")[0]['generated_text']\n",
    "    return result\n",
    " \n",
    "prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    " \n",
    "You are a helpful assistant<|eot_id|>\\n<|start_header_id|>user<|end_header_id|>\n",
    " \n",
    "대기 오염을 어떻게 줄일 수 있나요?<|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    " \n",
    "print(generate_and_stop(prompt))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
